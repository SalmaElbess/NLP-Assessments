{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW1_Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8P741JFB75"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFaoCGw-FC8z",
        "outputId": "93455ad7-b7d5-4c79-e0e4-b703418f35b1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpYDMfA_Jp2L"
      },
      "source": [
        "#loading index dict \n",
        "pickle_off = open(\"index.pkl\", 'rb')\n",
        "index_dict = pickle.load(pickle_off)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weRKORuW5wKa",
        "outputId": "ec3fa161-7712-4099-b36c-0ab9708bc8fe"
      },
      "source": [
        "query_list = [\" Who were famous women?\",\n",
        "              \"Poems and poets\",\n",
        "              \"What is the scientific work of islamic golden age? \",\n",
        "              \"Ancient Spain\",\n",
        "              \"A Famous Muslim Engineer\",\n",
        "              \"A woman with a kingdom\",\n",
        "              \"How were the prayer times determined?\"]\n",
        "for query in query_list:\n",
        "    print('Query: ', query)\n",
        "    print()\n",
        "    #lower case\n",
        "    lowercase_query = query.lower()\n",
        "\n",
        "    #tokenize\n",
        "    tokens = word_tokenize(lowercase_query)\n",
        "\n",
        "    #remove stop words\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    new_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    #remove panctuation \n",
        "    clear_tokens = [token for token in new_tokens if token.isalnum()]\n",
        "\n",
        "    #stemming \n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in clear_tokens]\n",
        "\n",
        "    # lemmatization \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    print('Parsed Tokens: ', lemmatized_tokens)\n",
        "    \n",
        "    #searching for cleaned tokens in the index dict\n",
        "\n",
        "    all_tokens_docs_set = set() #docs include any token in the query \n",
        "\n",
        "    for token in lemmatized_tokens:\n",
        "        found_list = index_dict.get(token,'Word Not Found')\n",
        "        docs_index = set([x[0] for x in found_list])\n",
        "        all_tokens_docs_set = all_tokens_docs_set.union(docs_index)\n",
        "        print('Token \"',token,'\" found in documents', docs_index )\n",
        "    print()\n",
        "    print('Query found in docs numbered:',all_tokens_docs_set )\n",
        "    print(\"-----------------\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query:   Who were famous women?\n",
            "\n",
            "Parsed Tokens:  ['famou', 'woman']\n",
            "Token \" famou \" found in documents {1, 9}\n",
            "Token \" woman \" found in documents {9, 4, 7}\n",
            "\n",
            "Query found in docs numbered: {1, 4, 7, 9}\n",
            "-----------------\n",
            "Query:  Poems and poets\n",
            "\n",
            "Parsed Tokens:  ['poem', 'poet']\n",
            "Token \" poem \" found in documents {3}\n",
            "Token \" poet \" found in documents {3, 6}\n",
            "\n",
            "Query found in docs numbered: {3, 6}\n",
            "-----------------\n",
            "Query:  What is the scientific work of islamic golden age? \n",
            "\n",
            "Parsed Tokens:  ['scientif', 'work', 'islam', 'golden', 'age']\n",
            "Token \" scientif \" found in documents {8, 2, 4, 5}\n",
            "Token \" work \" found in documents {0, 3, 4, 5}\n",
            "Token \" islam \" found in documents {0, 1, 2, 3, 4, 5, 8}\n",
            "Token \" golden \" found in documents {0, 2, 4, 5}\n",
            "Token \" age \" found in documents {0, 2, 4, 5}\n",
            "\n",
            "Query found in docs numbered: {0, 1, 2, 3, 4, 5, 8}\n",
            "-----------------\n",
            "Query:  Ancient Spain\n",
            "\n",
            "Parsed Tokens:  ['ancient', 'spain']\n",
            "Token \" ancient \" found in documents {2, 3, 4, 5}\n",
            "Token \" spain \" found in documents {8, 9, 3}\n",
            "\n",
            "Query found in docs numbered: {2, 3, 4, 5, 8, 9}\n",
            "-----------------\n",
            "Query:  A Famous Muslim Engineer\n",
            "\n",
            "Parsed Tokens:  ['famou', 'muslim', 'engin']\n",
            "Token \" famou \" found in documents {1, 9}\n",
            "Token \" muslim \" found in documents {2, 4, 5, 7, 8, 9}\n",
            "Token \" engin \" found in documents {0}\n",
            "\n",
            "Query found in docs numbered: {0, 1, 2, 4, 5, 7, 8, 9}\n",
            "-----------------\n",
            "Query:  A woman with a kingdom\n",
            "\n",
            "Parsed Tokens:  ['woman', 'kingdom']\n",
            "Token \" woman \" found in documents {9, 4, 7}\n",
            "Token \" kingdom \" found in documents {7}\n",
            "\n",
            "Query found in docs numbered: {9, 4, 7}\n",
            "-----------------\n",
            "Query:  How were the prayer times determined?\n",
            "\n",
            "Parsed Tokens:  ['prayer', 'time', 'determin']\n",
            "Token \" prayer \" found in documents {1}\n",
            "Token \" time \" found in documents {8, 4, 6}\n",
            "Token \" determin \" found in documents {4}\n",
            "\n",
            "Query found in docs numbered: {1, 4, 6, 8}\n",
            "-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5VK7AicMNp9"
      },
      "source": [
        "#### Assessment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyJc1GoeMTYt"
      },
      "source": [
        "**1. which query returned in exactly one document?**\n",
        "- None\n",
        "\n",
        "**2. which query returned almost all the documents?**\n",
        "- \"A Famous Muslim Engineer returned\" all docs except docs 3,6\n",
        "- \"What is the scientific work of islamic golden age?\" all docs except docs 6,7,9\n",
        "\n",
        "**3. which queries returned around 80% relevant results(more than one)?**\n",
        "- First \"Poems and poets\"\n",
        "- then \"What is the scientific work of islamic golden age?\"\n",
        "\n",
        "**4. which queries returned somehow irrelevant results?**\n",
        "- \"\"A Famous Muslim Engineer\"\n",
        "- \"How were the prayer times determined?\"\n",
        "- \"Ancient Spain\"\n",
        "- \"Who were famous women?\"\n",
        "- \"A Famous Muslim Engineer\"\n",
        "\n",
        "**5. why do you think some queries were successful in returning relevant results and others\n",
        "are not?**\n",
        "- This method of searching is not the most effecient, as there is some words are more important than others\n",
        "- The combinantions of some words is meaningful but the indvidual words are irrelevant like \"\"A Famous Muslim Engineer\" and \"\"A Famous Muslim Engineer\"\n",
        "- The queries that could get relevant data used individual words than will only exist in relevant results like \"Poems and poets\"\n",
        "and other was generic queries that already have relevant information in most of the docs like \"What is the scientific work of islamic golden age?\"\n",
        "- it will be better to find the docs that are common between all the query tokens and if possible, prioritize the tokens. In addition, we can order the results in a way such that the docs with higher query tokens frequency will appear first. We can even put a threshold if the frequency of tokens is below that limit, the doc will not appear in the results. \n",
        "\n",
        "**6. if you read any of the documents which character was totally,new for you,what new\n",
        "information did you get**\n",
        "- Wallada bint al-Mustakfi - doc 6 \n",
        "- Is is my first incounter with female poet from the islamic age and this \"There she offered instruction in poetry and the arts of love to women of all classes, from those of noble birth to slaves purchased by Wallada herself.\" is great :D. I did not like before how old poems were moslty written by men to women or other men, as I did not find something to relate to and I understand the importance of art. So, great news ^-^\n",
        "\n",
        "\n"
      ]
    }
  ]
}
